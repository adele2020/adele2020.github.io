---
title:  "[CS231n]Image Classification(이미지 분류)"
excerpt: "Deep Learning for Computer Vision Lecture 2"
categories:
  - SSACxAIFFEL
tags:
  - CS231n

year-archive: 2021
last_modified_at: 2021-01-04T11:50:00-05:00
---

## Image Classification
이미지 분류 - 입력 이미지를 미리 정해진 카테고리 중 하나인 라벨로 분류하는 문제

### Problem :

Semantic Gap   
사용자가 원하는 이미지와 검색된 결과 이미지 사이에 의미적 차이가 나는 것을 말한다.


### Challenge :
이미지를 분류하는 일이 사람에게는 대수롭지 않겠지만, 컴퓨터 비전의 관점에서 생각해보면 해결해야 하는 문제들이 많다.
고양이를 예로 들자면,

+ `Viewpoint Variation` 관점 변화  
ex) 한 고양이의 어디에 집중했냐에 따라 카메라에 의해 시점이 달라질 수 있다.  
+ `intraclass Variation` 클래스내의 변이,변화,변동  
ex) 같은 고양이지만 생김새가 다르다.  
+ Fine-Grained Categories 세분화된 카테고리  
ex) 고양이 안에서도 종류가 다양하다.  
+ `Background Clutter` 배경 혼란  
ex) 이미지를 찍은 배경이 고양이를 잘 드러내지 않을 수 있다.  
+ `Illumination Changes` 조명 변경  
ex) 조명에 따라 고양이가 달라 보일 수 있다.  
+ `Deformation` 흉한 모습  
ex) 고양이인데 이상한 포즈로 사람같이 보일 수 있다.  
+ `Occlusion` 폐색  
ex) 고양이가 숨어서 꼬리만 보일 수 있다.  

### Image Classification Useful :
이미지 분류 사용 예

- `Medical Imaging` 의료 영상

- `Galaxy Classification` 은하 분류

- `Whale recognition` 고래 인식

- `Building Block for other tasks` 다른 작업을 위한 빌딩 블록
    - Object Detection 물체감지
    - Image Captioning 이미지 캡션
    - Playing Go 바둑 재생

### Machine Learning : Data-Driven Approach
코드를 통해 직접적으로 모든 것을 카테고리로 분류하기 보다는 좀 더 쉬운 방법을 사용. 컴퓨터에게 각 클래스에 대해 많은 예제를 주고 나서 이 예제들을 보고 시각적으로 학습할 수 있는 학습 알고리즘을 개발

1. 이미지 데이터 셋과 라벨을 수집하고,
2. 분류기를 학습하기 위해 머신러닝을 사용하고,
3. 그 분류기를 새 이미지로 평가하고.

### Image Classification Datasets :
이미지 분류를 연구해 볼 수 있는 여러가지 이미지 데이터 셋이 제공되고 있다.

+ *MNIST* - 10 classes (Digits 0 to 9), 28x28 grayscale	images
+ *CIFAR10* - 10 classes (32x32 RGB images)

+ *CIFAR100* - 100 classes (32x32 RGB images, 20 superclasses with 5 classes each)

+ *ImageNet* - 1000 classes

+ *MIT Places(Places365)* - 365 classes of different scene types

+ *Omniglot* - 1623 categories, 20 images per category

### Distance Metric

L1 (Manhattan) - 각 변수 값 차이의 절대값의 합

L2 (Euclidean) - 제곱의 합에 루트

<!--- 그외 거리 측정법은 무엇이 있을까?-->

### Setting Hyperparameters   

- train data / validation data / test data

- 교차 검증 
cross validation k folds (k겹 교차검증)

## KNN

KNN 모델은 모든 인스턴스들(관측치들)이 특정 n-차원 공간에 있으며 “거리” 관점에서 이웃들을 정의한다는 것이다.

K = 이웃들의 수  
객체는 k개의 최근접 이웃 사이에서 과반수 의결에 의해 분류
된다
<!--
### 데이터 범주의 예측
- Majority voting(다수결) : 명목형, 이웃 범주 가운데 빈도기준 제일 많은 것을 새 범주로 예측
- Weighted voting(가중치) : 거리가 가까운(유사도가 높은)이우에 좀 더 가중
-->
### 특징

- 만약 K의 숫자가 너무 작으면 지역적 특성을 지나치게 반영한다.
반대의 경우로 K의 숫자가 클 경우
데이터가 underfitting 될 가능성이 있다.
- KNN은 좋은 알고리즘은 아니지만 k와 distance metric을 잘 설정하면 좋은 결과를 내놓을 수 있다.  유사 논문 검색에서는 KNN이 좋은 성능을 보임.
- 변수별로 평균과 분산이 다르면 거리계산에 혼동이 생길 수 있기 때문에 KNN 수행 전 반드시 정규화 필요.
- 신규 데이터가 들어왔을 때 기존 데이터들과의 거리로 이웃들 간의 관계 파악 뒤 값을 결정
- 학습이 없다! 게으른 모델(Lazy Model)
목적은 신규 데이터 분류(회귀) 예측에 대해 맞힐 수 있는 사전 데이터베이스를 구축하는 것임
- Instance(관측치) based Learning
  학습 모델을 별도로 구축하지 않는 방식
  각각의 instance(관측치) 기준으로 분류 또는 회귀를 수행
  (그와 비교하여 Model based learning은 학습 모델을 생성, 분류 또는 회귀를 수행)

### 장점

알고리즘이 간단하여 구현하기 쉽다   
수치 기반 데이터 분류 작업에서 성능이 좋다

### 단점

학습 데이터의 양이 많으면 분류 속도가 느려진다  
차원(벡터)의 크기가 크면 계산량이 많아진다  
사용자가 직접 K를 결정해야하는 어려움이 있다  

### KNN 스크립트  
 `(준비중)`

> `KNN에 대한 내용은 추가 정리했음.`
