---
title:  "[11주차] Aiffel에서 인공지능 도전기"
excerpt: ""
categories:
  - SSACxAIFFEL
tags:
  - 주간정리

year-archive: 2021
last_modified_at: 2021-03-19T17:15:00-00:00
---

## FUNDAMENTALS
#### 28. 파이썬으로 데이터 전처리 웹 만들기
파이썬의 마이크로 웹 프레임워크인 Flask를 이해하고, 이를 이용하여 간단한 이미지, SQL 전처리 웹 어플리케이션을 제작해 본다.
#### 29. 활성화 함수의 이해
퍼셉트론에서 선형과 비선형이 가지는 의미 및 비선형함수가 필요한 이유를 파악하고, 다양한 활성화 함수에 대해 알아본다.
#### 30. 딥네트워크, 서로 뭐가 다른 거죠?
Computer Vision에 주로 사용되는 Convolutional Neural Network의 주요 개념 및 널리 사용되는 VGG, ResNet 등 네트워크 구조에 대해 알아본다.
>`파이썬의 경량 프레임워크인 Flask로 웹을 만들었다. 생각만큼 제대로 되진 않았다. 29.노드에서는 활성화 함수를 배웠고, 30.노드에서는 딥네트워크를 배웠다. 배웠던 내용이 또 나오니 익숙한 얼굴을 본 듯 반갑기도 하다. `

## EXPLORATION  
#### 19. 인간보다 퀴즈를 잘푸는 인공지능 [>>프로젝트 보기](https://github.com/adele2020/ssacxaiffel/blob/main/%5BE13%5D_Stock_forecast.ipynb)
BERT 모델을 이용한 Q&A 태스크 해결을 수행하며, 자연어처리 분야의 pretrained model의 효용성에 대해 이해해 본다.
#### 20. 난 스케치를 할 테니 너는 채색을 하거라 [>>프로젝트 보기](https://github.com/adele2020/ssacxaiffel/blob/main/%5BE14%5D_medical_images_cnn.ipynb)
이미지 생성 모델로 사용되는 GAN 중에서 조건이 추가된 cGAN에 대해 알아보고 Pix2Pix를 배워봅니다.
>`NLP쪽으로는 BERT를 배웠고, CV쪽으로는 Pix2Pix를 배웠다. 노드를 할 때마다 딥러닝이 어떻게 진화하고 있는지 알 수 있다. 좇아가기 힘들다. 헉헉... 겨우 왔나 싶으면 아마도 딥러닝 기술은 저 멀리 가 있을 거 같다. `  

## DeepML   
CS231n Lecture 13: Attention
>`노드에서 배웠던 Attention과 Transformer를 다시 마주하게 되었다. 자신의 말로 Attention 설명하기를 했다. 이렇게 문답을 하니 공부가 되는거 같다.`
1. seq2seq 문제점 2개가 무엇인가요?
- input data 와 output data의 길이가 길어지게 되면 데이터가 순차적으로 전달되면서 정보를 잊어버리는 현상 즉, 오래된 기억은 잊어버리게 되는 long term dependency 현상이 발생하게 된다.
- 또 하나는 신경망의 gradient를 구하는 과정인 backpropergation을 이용하게 되는데 이때 발생하는 vanishing gradient 와 exploding gradient의 문제점을 들 수 있다. 학습하는 과정중 gradient가 작은 값 또는 큰 값이 계산되었을 경우 gradient가 곱해지면서 소실되거나 폭발하는 점을 문제점으로 들 수 있다.
2. Query, Key, Value가 무슨 뜻인지 설명해 주세요.
dictionary 자료구조에서 query 와 key가 일치하면 value를 return 하는 것에 비유할 수 있다.
attention에서는 Query와 key의 유사도를 계산 한 후 value의 가중합을 계산하게 되는데 해결하려는 문제가 무엇이냐에 따라 Key 와 Query, Value가  다를 수 있다.  
3. attention 함수의 계산 과정을 순차적으로 설명해 주세요.
- query 와 key의 유사도를 계산하는 과정
- 유사도와 value를 곱하는 과정
- 유사도와 value를 곱한 값의 가중합을 리턴하는 과정
4. RNN과 CNN에 attention을 적용했을 때 어떤 문제가 발생했나요?
- RNN은 sequential data를 다루다 보니 병렬 처리가 불가능하고, 문장 내에 sequential length가 길어지면 계산 시간이 길어지고, 복잡도가 올라간다. vanishing gradient 와 long term dependency가 발생하다.
- CNN은 하이퍼파라미터로 window size를 결정하게 되는데 long-range dependencies 문장에서는 convolution layer를 많이 쌓아야하는 문제가 발생한다. long path length 문제이다.
5. self attention이 무엇인지 설명해 주세요.
RNN, CNN구조를 사용하지 않고, attention만을 사용하여 task를 수행하고 feature를 표현한 것
6. transformer가 기여한 점은 무엇인가요?
기존의 RNN과 CNN을 쓰지 않기 때문에 recurrent, convolution 연산을 하지 않는다. 그래서 전체적인 계산의 복잡도가 dot product라는 심플한 연산만을 사용하기 때문에 감소했고, 병렬 처리가 가능해졌으며, long range path를 가지고 있던 문제를 해결했다. attention의 과정을 시각화 하기 쉬워져 더욱 더 해석 가능한 모델이 되었다.

## 코딩마스터   
Do it! 자료구조와 알고리즘 입문: 4. 스택과 큐
>`스택을 배웠다. 큐는 다음 주에 진행된다. 스택은 LIFO로 후입선출 방식의 자료구조이다. 이전 코마반에서 발표했던 내용이라 어렵진 않았다. 코드 구현 부분을 정리하고 git에 남겼다.`  

## 일주일 간의 느낀 점
공부를 할 때 이끌어 주는 사람이 누구냐에 따라서 많이 달라지는 거 같다.
