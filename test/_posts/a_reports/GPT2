Language Models are Unsupervised Multitask Learners

Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.
We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.
When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.
The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.
Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText.
Samples from the model reflect these improvements and contain coherent paragraphs of text.
These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.

질문 답변, 기계 번역, 독해 및 요약과 같은 자연어 처리 작업은 일반적으로 작업 별 데이터 세트에 대한지도 학습을 통해 접근합니다.
WebText라고하는 수백만 개의 웹 페이지로 구성된 새로운 데이터 세트에서 학습 할 때 언어 모델이 명시적인 감독없이 이러한 작업을 학습하기 시작 함을 보여줍니다.
문서와 질문을 조건으로 할 때 언어 모델에서 생성 된 답변은 CoQA 데이터 세트에서 55F1에 도달합니다. 127,000 개 이상의 교육 예제를 사용하지 않고 기준 시스템 4 개 중 3 개와 일치하거나 초과합니다.
언어 모델의 용량은 제로 샷 작업 전송의 성공에 필수적이며 이를 증가 시키면 작업 전반에 걸쳐 로그 선형 방식으로 성능이 향상됩니다.
가장 큰 모델 인 GPT-2는 제로 샷 설정에서 테스트 된 언어 모델링 데이터 세트 8 개 중 7 개에서 최첨단 결과를 달성하지만 여전히 WebText에 맞지 않는 1.5B 매개 변수 Transformer입니다.
모델의 샘플은 이러한 개선 사항을 반영하고 일관된 텍스트 단락을 포함합니다.
이러한 발견은 자연스럽게 발생하는 시연에서 작업을 수행하는 방법을 배우는 언어 처리 시스템을 구축하는 유망한 경로를 제안합니다.
