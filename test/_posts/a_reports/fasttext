Enriching Word Vectors with Subword Information

Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks.
Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word.
This is a limitation, especially for languages with large vocabularies and many rare words.
In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.
A vector representation is associated to each character n-gram; words being represented as the sum of these representations.
Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data.
We evaluate our word representations on nine different languages, both on word similarity and analogy tasks.
By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.

레이블이없는 큰 말뭉치에 대해 훈련 된 연속 단어 표현은 많은 자연어 처리 작업에 유용합니다.
이러한 표현을 학습하는 인기 모델은 각 단어에 고유 한 벡터를 할당하여 단어의 형태를 무시합니다.
이는 특히 어휘가 많고 희귀 단어가 많은 언어의 경우 제한 사항입니다.
이 논문에서 우리는 각 단어가 문자 n-gram의 가방으로 표현되는 skipgram 모델을 기반으로 한 새로운 접근 방식을 제안합니다.
벡터 표현은 각 문자 n-gram과 연관됩니다; 이러한 표현의 합으로 표현되는 단어.
우리의 방법은 빠르기 때문에 큰 말뭉치에서 모델을 빠르게 훈련시킬 수 있고 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산할 수 있습니다.
우리는 단어 유사성 및 유추 작업 모두에서 9 개의 다른 언어로 단어 표현을 평가합니다.
최근에 제안 된 형태 학적 단어 표현과 비교하여 벡터가 이러한 작업에 대해 최첨단 성능을 달성 함을 보여줍니다.
