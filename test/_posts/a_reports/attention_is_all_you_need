Attention Is All You Need

The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.
The best performing models also connect the encoder and decoder through an attention mechanism.
We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.
Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.
On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.
We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

지배적 인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 반복 또는 컨벌루션 신경망을 기반으로합니다.
최고 성능의 모델은주의 메커니즘을 통해 인코더와 디코더를 연결하기도합니다.
우리는주의 메커니즘만을 기반으로하는 새로운 단순 네트워크 아키텍처 인 Transformer를 제안하며, recurrence와  컨볼 루션을 완전히 배제합니다.
두 가지 기계 번역 작업에 대한 실험을 통해 이러한 모델은 품질이 우수하면서도 병렬화가 가능하고 학습 시간이 훨씬 단축되는 것으로 나타났습니다.
우리의 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고의 결과를 2BLEU 이상 향상 시켰습니다.
WMT 2014 영어-프랑스어 번역 작업에서, 우리 모델은 8 개의 GPU에서 3.5 일 동안 훈련 한 후 새로운 단일 모델 최신 BLEU 점수 41.8을 설정합니다. 이는 최고의 훈련 비용의 작은 부분입니다. 문학의 모델.
Transformer가 크고 제한된 학습 데이터를 사용하여 영어 구성 구문 분석에 성공적으로 적용하여 다른 작업에 잘 일반화됨을 보여줍니다.
