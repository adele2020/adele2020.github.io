Neural Machine Translation of Rare Words with Subword Units

Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem.
Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary.
In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units.
This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).
We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.

신경 기계 번역 (NMT) 모델은 일반적으로 고정 된 어휘로 작동하지만 번역은 개방형 어휘 문제입니다.
이전 작업은 사전에 뒤로 물러서 어휘 외 단어의 번역을 다룹니다.
이 논문에서는 희귀하고 알려지지 않은 단어를 하위 단어 단위의 시퀀스로 인코딩하여 개방형 어휘 번역이 가능한 NMT 모델을 만드는 더 간단하고 효과적인 접근 방식을 소개합니다.
이것은 예를 들어 이름 (문자 복사 또는 음역을 통해), 화합물 (작문 번역을 통해), 동음 및 외래어 (음운 및 형태 학적 변형을 통해)와 같이 단어보다 작은 단위를 통해 다양한 단어 클래스를 번역 할 수 있다는 직관을 기반으로합니다.
간단한 문자 n-gram 모델과 바이트 쌍 인코딩 압축 알고리즘을 기반으로 한 세분화를 포함하여 다양한 단어 세분화 기술의 적합성에 대해 논의하고, WMT 15 번역 작업에 대한 백 오프 사전 기준선보다 하위 단어 모델이 향상됨을 경험적으로 보여줍니다. → 독일어 및 영어 → 러시아어 각각 최대 1.1 및 1.3BLEU.

신경 기계 번역(NMT) 모델은 일반적으로 고정된 어휘로 작동하지만, 번역은 개방형 어휘 문제이다.
이전 작업은 사전으로 돌아가 어휘를 벗어난 단어의 번역을 다룬다.
본 논문에서, 우리는 희귀하고 알려지지 않은 단어를 서브워드 단위의 시퀀스로 인코딩함으로써 NMT 모델을 오픈 어휘 번역이 가능하게 하는 더 간단하고 효과적인 접근 방식을 소개한다.
이는 다양한 단어 클래스가 단어보다 작은 단위, 예를 들어 이름(문자 복사 또는 번역), 복합어(구성 번역을 통해), 인지 및 외래어(음운학적 및 형태학적 변환을 통해)를 통해 번역될 수 있다는 직관에 기초한다.
우리는 간단한 문자 n-그램 모델과 바이트 쌍 인코딩 압축 알고리듬을 기반으로 한 분할을 포함한 다양한 단어 분할 기술의 적합성에 대해 논의하고, 하위 단어 모델이 영어에서 독일어, 영어로에서 러시아로 WMT 15 번역 작업에 대한 백오프 사전 기준보다 향상된다는 것을 경험적으로 보여준다.각각 최대 1.1 BLEU와 1.3 BLEU의 이안입니다.
